{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# week 11: Recurrent neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次实验设计主要修改自Pytorch官方提供的教程，在官方的教程上进行了整合。如对本次课件的内容有任何疑惑的同学可以直接微信我或者邮件到cuizhiying.csu@gmail.com    \n",
    "**原作者**: `Sean Robertson <https://github.com/spro/practical-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Experimental content and requirements\n",
    "本次实验内容主要分为词语分类和词语生成两大部分，具体要求如下：\n",
    "1. 体验语义分割网络的运行过程，和基本的代码结构，结合理论课的内容，加深对RNN的思考和理解\n",
    "2. 独立完成实验指导书中提出的问题（简要回答）\n",
    "3. 按照实验指导书的引导，填充缺失部分的代码，让程序顺利地运转起来\n",
    "4. 坚持独立完成，**禁止抄袭**\n",
    "5. 实验结束后，将整个文件夹下载下来（注意保留程序运行结果），打包上传到超算课堂网站中（统一使用zip格式压缩）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 0.2 Recommended Reading\n",
    "\n",
    "These are all good additions to the understanding of RNN.:\n",
    "\n",
    "-  [The Unreasonable Effectiveness of Recurrent Neural\n",
    "   Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "   shows a bunch of real life examples\n",
    "-  [Understanding LSTM\n",
    "   Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "   is about LSTMs specifically but also informative about RNNs in\n",
    "   general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Classifying Names with a Character-Level RNN\n",
    "*********************************************\n",
    "We will be building and training a basic character-level RNN to classify\n",
    "words. A character-level RNN reads words as a series of characters -\n",
    "outputting a prediction and \"hidden state\" at each step, feeding its\n",
    "previous hidden state into each next step. We take the final prediction\n",
    "to be the output, i.e. which class the word belongs to.\n",
    "\n",
    "Specifically, we'll train on a few thousand surnames from 18 languages\n",
    "of origin, and predict which language a name is from based on the\n",
    "spelling:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.1 Preparing the Data\n",
    "\n",
    "Download the data from [here](https://download.pytorch.org/tutorial/data.zip) and extract it to the current directory.\n",
    "\n",
    "Included in the ``data/names`` directory are 18 text files named as\n",
    "\"[Language].txt\". Each file contains a bunch of names, one name per\n",
    "line, mostly romanized (but we still need to convert from Unicode to\n",
    "ASCII).\n",
    "\n",
    "We'll end up with a dictionary of lists of names per language,\n",
    "``{language: [names ...]}``. The generic variables \"category\" and \"line\"\n",
    "(for language and name in our case) are used for later extensibility.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/names/Russian.txt', 'data/names/Portuguese.txt', 'data/names/German.txt', 'data/names/Polish.txt', 'data/names/French.txt', 'data/names/Spanish.txt', 'data/names/Chinese.txt', 'data/names/Czech.txt', 'data/names/Greek.txt', 'data/names/Scottish.txt', 'data/names/Dutch.txt', 'data/names/Arabic.txt', 'data/names/Japanese.txt', 'data/names/Vietnamese.txt', 'data/names/Korean.txt', 'data/names/Irish.txt', 'data/names/English.txt', 'data/names/Italian.txt']\n",
      "Slusarski\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "print(findFiles('data/names/*.txt'))\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "print(unicodeToAscii('Ślusàrski'))\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "all_categories = []\n",
    "category_lines = {}\n",
    "\n",
    "# Split it into training set and validation set\n",
    "training_lines = {}\n",
    "validation_lines = {}\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "    \n",
    "    num_of_training_set = int(len(lines)*0.8)\n",
    "    training_lines[category]   = lines[:num_of_training_set]\n",
    "    validation_lines[category] = lines[num_of_training_set:] \n",
    "\n",
    "n_categories = len(all_categories)\n",
    "print(n_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have ``category_lines``, a dictionary mapping each category\n",
    "(language) to a list of lines (names). We also kept track of\n",
    "``all_categories`` (just a list of languages) and ``n_categories`` for\n",
    "later reference.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']\n"
     ]
    }
   ],
   "source": [
    "print(category_lines['Italian'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Turning Names into Tensors\n",
    "\n",
    "\n",
    "Now that we have all the names organized, we need to turn them into\n",
    "Tensors to make any use of them.\n",
    "\n",
    "To represent a single letter, we use a \"one-hot vector\" of size\n",
    "``<1 x n_letters>``. A one-hot vector is filled with 0s except for a 1\n",
    "at index of the current letter, e.g. ``\"b\" = <0 1 0 0 0 ...>``.\n",
    "\n",
    "To make a word we join a bunch of those into a 2D matrix\n",
    "``<line_length x 1 x n_letters>``.\n",
    "\n",
    "That extra 1 dimension is because PyTorch assumes everything is in\n",
    "batches - we're just using a batch size of 1 here.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "torch.Size([5, 1, 57])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Tensor here, someone else may call it vector. \n",
    "print(letterToTensor('J'))\n",
    "\n",
    "print(lineToTensor('Jones').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Creating the Network\n",
    "\n",
    "\n",
    "Before autograd, creating a recurrent neural network in Torch involved\n",
    "cloning the parameters of a layer over several timesteps. The layers\n",
    "held hidden state and gradients which are now entirely handled by the\n",
    "graph itself. This means you can implement a RNN in a very \"pure\" way,\n",
    "as regular feed-forward layers.\n",
    "\n",
    "Inside the forward function, we need to Loop \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![figure](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BaseRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(BaseRNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # input to hidden\n",
    "        self.i2h = nn.Linear(input_size,  hidden_size)\n",
    "        # hidden to hidden\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "        #############################################\n",
    "        #\n",
    "        #  Change your activation function here\n",
    "        #\n",
    "        #############################################\n",
    "        self.activation = nn.Tanh()\n",
    "        # hidden to output\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def step(self, letter, hidden):\n",
    "        i2h = self.i2h(letter)\n",
    "        h2h = self.h2h(hidden)\n",
    "        hidden = self.activation( h2h+i2h )\n",
    "        \n",
    "        output = self.h2o(hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def forward(self, word):\n",
    "        hidden = self.initHidden()\n",
    "        for i in range(word.size()[0]):\n",
    "            # Only the last output will be used to predict\n",
    "            output, hidden = self.step(word[i], hidden)\n",
    "        return output\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "n_hidden = 128\n",
    "rnn = BaseRNN(n_letters, n_hidden, n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#\n",
    "# Finish a the following model, and train it\n",
    "#\n",
    "###############################################################################\n",
    "\n",
    "class DeeperRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DeeperRNN, self).__init__()\n",
    "        self.hidden1_size = hidden_size\n",
    "        self.hidden2_size = hidden_size\n",
    "        self.layer1 = BaseRNN(input_size, hidden_size, output_size)\n",
    "        self.layer2 = BaseRNN(hidden_size, hidden_size, output_size)\n",
    "        \n",
    "    def step(self, letter, hidden1, hidden2):\n",
    "        output1, hidden1 = self.layer1.step(letter, hidden1)\n",
    "        output2, hidden2 = self.layer2.step(hidden1, hidden2)\n",
    "        return output2, hidden1, hidden2\n",
    "    \n",
    "    def forward(self, word):\n",
    "        hidden1, hidden2 = self.initHidden()\n",
    "        for i in range(word.size()[0]):\n",
    "            output, hidden1, hidden2 = self.step(word[i], hidden1, hidden2)\n",
    "        return output\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden1_size), torch.zeros(1, self.hidden2_size)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "# Remember to uncomment the following two line after you finish your model and\n",
    "# start to retrain your model\n",
    "\n",
    "###############################################################################\n",
    "#n_hidden = 128\n",
    "#rnn = DeeperRNN(n_letters, n_hidden, n_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a step of this network we need to pass an input (in our case, the\n",
    "Tensor for the current letter) and a previous hidden state (which we\n",
    "initialize as zeros at first). We'll get back the output (probability of\n",
    "each language) and a next hidden state (which we keep for the next\n",
    "step).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = letterToTensor('A')\n",
    "hidden =torch.zeros(1, n_hidden)\n",
    "\n",
    "output, next_hidden = rnn.step(input, hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of efficiency we don't want to be creating a new Tensor for\n",
    "every step, so we will use ``lineToTensor`` instead of\n",
    "``letterToTensor`` and use slices. This could be further optimized by\n",
    "pre-computing batches of Tensors.\n",
    "\n",
    "\n",
    "Each loop in side forward function will:\n",
    "\n",
    "-  Create input and target tensors\n",
    "-  Create a zeroed initial hidden state\n",
    "-  Read each letter in and\n",
    "\n",
    "   -  Keep hidden state for next letter\n",
    "\n",
    "-  Compare final output to target\n",
    "-  Back-propagate\n",
    "-  Return the output and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "tensor([[ 0.0293,  0.1889, -0.1307, -0.0204,  0.1203,  0.1140, -0.2874,\n",
      "         -0.1591,  0.0987, -0.0450,  0.1216, -0.1051,  0.0731, -0.0772,\n",
      "         -0.1894,  0.0963,  0.0500, -0.0036]])\n",
      "torch.Size([1, 18])\n"
     ]
    }
   ],
   "source": [
    "input = lineToTensor('Albert')\n",
    "print(n_hidden)\n",
    "\n",
    "output = rnn(input)\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the output is a ``<1 x n_categories>`` Tensor, where\n",
    "every item is the likelihood of that category (higher is more likely).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Training\n",
    "\n",
    "#### 1.4.1 Preparing for Training\n",
    "Before going into training we should make a few helper functions. The\n",
    "first is to interpret the output of the network, which we know to be a\n",
    "likelihood of each category. We can use ``Tensor.topk`` to get the index\n",
    "of the greatest value:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Portuguese', 1)\n"
     ]
    }
   ],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "print(categoryFromOutput(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also want a quick way to get a training example (a name and its\n",
    "language):\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category = Portuguese / line = Cardozo\n",
      "category = Japanese / line = Kanada\n",
      "category = Vietnamese / line = Phan\n",
      "category = German / line = Herbert\n",
      "category = French / line = Laurent\n",
      "category = English / line = Pearce\n",
      "category = English / line = Harries\n",
      "category = Vietnamese / line = Mach\n",
      "category = Arabic / line = Antar\n",
      "category = Chinese / line = Chin\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    # attention: split training set \n",
    "    line = randomChoice(training_lines[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "def randomValidationExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    # attention: split validation set\n",
    "    line = randomChoice(validation_lines[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    print('category =', category, '/ line =', line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 Training the Network\n",
    "\n",
    "Now all it takes to train this network is show it a bunch of examples,\n",
    "have it make guesses, and tell it if it's wrong.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "def train(category_tensor, line_tensor):\n",
    "    output = rnn(line_tensor)\n",
    "    rnn.zero_grad()\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        if hasattr(p.grad, \"data\"):\n",
    "            p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just have to run that with a bunch of examples. Since the\n",
    "``train`` function returns both the output and loss we can print its\n",
    "guesses and also keep track of loss for plotting. Since there are 1000s\n",
    "of examples we print only every ``print_every`` examples, and take an\n",
    "average of the loss.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 2% (0m 5s) 2.3374 Bradan / Scottish ✗ (Irish)\n",
      "10000 5% (0m 9s) 1.8773 Alvarez / German ✗ (Spanish)\n",
      "15000 7% (0m 14s) 3.1857 Gaspar / Arabic ✗ (Spanish)\n",
      "20000 10% (0m 18s) 0.9884 Blanchett / French ✓\n",
      "25000 12% (0m 23s) 1.6979 Garland / Scottish ✗ (English)\n",
      "30000 15% (0m 29s) 0.1583 Airaldi / Italian ✓\n",
      "35000 17% (0m 36s) 0.2246 Ceallach / Irish ✓\n",
      "40000 20% (0m 43s) 0.2934 Yong / Chinese ✓\n",
      "45000 22% (0m 49s) 1.1266 Mcnab / Irish ✓\n",
      "50000 25% (0m 55s) 0.4578 Doan / Vietnamese ✓\n",
      "55000 27% (1m 2s) 0.8775 Cloutier / French ✓\n",
      "60000 30% (1m 9s) 2.6118 Judd / Scottish ✗ (English)\n",
      "65000 32% (1m 15s) 0.7079 Avana / Spanish ✓\n",
      "70000 35% (1m 21s) 0.2145 Castro / Portuguese ✓\n",
      "75000 37% (1m 27s) 2.8793 Manus / Arabic ✗ (Irish)\n",
      "80000 40% (1m 33s) 2.6163 Mai / Chinese ✗ (Vietnamese)\n",
      "85000 42% (1m 40s) 0.0278 Doan / Vietnamese ✓\n",
      "90000 45% (1m 46s) 0.1272 Dogilev / Russian ✓\n",
      "95000 47% (1m 53s) 0.0840 Zhong / Chinese ✓\n",
      "100000 50% (1m 59s) 0.3138 Puscharovsky / Russian ✓\n",
      "105000 52% (2m 6s) 0.4175 O'Reilly / Irish ✓\n",
      "110000 55% (2m 12s) 0.0574 Acardi / Italian ✓\n",
      "115000 57% (2m 19s) 0.6178 Glavinsky / Russian ✓\n",
      "120000 60% (2m 26s) 4.8215 Names / Arabic ✗ (Irish)\n",
      "125000 62% (2m 33s) 0.6204 Le / Vietnamese ✓\n",
      "130000 65% (2m 39s) 0.2503 Pinho / Portuguese ✓\n",
      "135000 67% (2m 46s) 2.8410 Monfort / French ✗ (Czech)\n",
      "140000 70% (2m 53s) 0.8803 Jeong / Chinese ✗ (Korean)\n",
      "145000 72% (3m 0s) 0.0219 O'Sullivan / Irish ✓\n",
      "150000 75% (3m 7s) 0.0634 Niemczyk / Polish ✓\n",
      "155000 77% (3m 14s) 0.4387 Reynder / Dutch ✓\n",
      "160000 80% (3m 20s) 1.6852 Linden / English ✗ (German)\n",
      "165000 82% (3m 27s) 0.7880 Monfort / Czech ✓\n",
      "170000 85% (3m 33s) 0.0655 Gerges / Arabic ✓\n",
      "175000 87% (3m 39s) 0.0308 Issa / Arabic ✓\n",
      "180000 90% (3m 46s) 0.4360 Toma / Arabic ✓\n",
      "185000 92% (3m 53s) 0.0001 Metrofanis / Greek ✓\n",
      "190000 95% (4m 0s) 0.0041 Kahaya / Japanese ✓\n",
      "195000 97% (4m 7s) 0.0541 Deniau / French ✓\n",
      "200000 100% (4m 13s) 2.1004 Acqua / Spanish ✗ (Italian)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "n_iters = 200000\n",
    "print_every = 5000\n",
    "plot_every = 1000\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output, loss = train(category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.3 Plotting the Results\n",
    "--------------------\n",
    "\n",
    "Plotting the historical loss from ``all_losses`` shows the network\n",
    "learning:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#################################################\n",
    "#\n",
    "# Tips: your could use plt.plot\n",
    "#\n",
    "#################################################\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Evaluating the Results\n",
    "\n",
    "To see how well the network performs on different categories, we will\n",
    "create a confusion matrix, indicating for every actual language (rows)\n",
    "which language the network guesses (columns). To calculate the confusion\n",
    "matrix a bunch of samples are run through the network with\n",
    "``evaluate()``, which is the same as ``train()`` minus the backprop.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traing set Acc is 0.7975999712944031\n",
      "validation set Acc is 0.3752000033855438\n"
     ]
    }
   ],
   "source": [
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion_training   = torch.zeros(n_categories, n_categories)\n",
    "confusion_validation = torch.zeros(n_categories, n_categories)\n",
    "n_confusion = 5000\n",
    "\n",
    "# Just return an output given a line\n",
    "def evaluate(line_tensor):\n",
    "    rnn.eval()\n",
    "    output = rnn(line_tensor)\n",
    "    return output\n",
    "\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(n_confusion):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output = evaluate(line_tensor)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "    category_i = all_categories.index(category)\n",
    "    confusion_training[category_i][guess_i] += 1\n",
    "\n",
    "    \n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(n_confusion):\n",
    "    category, line, category_tensor, line_tensor = randomValidationExample()\n",
    "    output = evaluate(line_tensor)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "    category_i = all_categories.index(category)\n",
    "    confusion_validation[category_i][guess_i] += 1\n",
    "    \n",
    "    \n",
    "# catcul acc\n",
    "right_train = 0\n",
    "right_valid = 0\n",
    "for i in range(n_categories):\n",
    "    right_train += confusion_training[i][i]\n",
    "    right_valid += confusion_validation[i][i]\n",
    "acc_train = right_train / n_confusion\n",
    "acc_valid = right_valid / n_confusion\n",
    "\n",
    "# Normalize by dividing every row by its sum and \n",
    "for i in range(n_categories):\n",
    "    confusion_training[i] = confusion_training[i] / confusion_training[i].sum()\n",
    "    confusion_validation[i] = confusion_validation[i] / confusion_validation[i].sum()\n",
    "\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(121)\n",
    "cax1 = ax1.matshow(confusion_training.numpy())\n",
    "#fig.colorbar(cax1)\n",
    "ax2 = fig.add_subplot(122)\n",
    "cax2 = ax2.matshow(confusion_validation.numpy())\n",
    "#fig.colorbar(cax2)\n",
    "\n",
    "# Set up axes\n",
    "ax1.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax1.set_yticklabels([''] + all_categories)\n",
    "ax2.set_xticklabels([''] + all_categories, rotation=90)\n",
    "#ax2.set_yticklabels([''] + all_categories)\n",
    "\n",
    "# Force label at every tick\n",
    "#ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "#ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "plt.show()\n",
    "\n",
    "print(\"Traing set Acc is\", acc_train.item())\n",
    "print(\"validation set Acc is\", acc_valid.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pick out bright spots off the main axis that show which\n",
    "languages it guesses incorrectly, e.g. Chinese for Korean, and Spanish\n",
    "for Italian. It seems to do very well with Greek, and very poorly with\n",
    "English (perhaps because of overlap with other languages).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Running on User Input\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> Dovesky\n",
      "Probability (0.07) Scottish\n",
      "Probability (0.06) Portuguese\n",
      "Probability (0.06) Irish\n",
      "\n",
      "> Jackson\n",
      "Probability (0.07) Irish\n",
      "Probability (0.07) Scottish\n",
      "Probability (0.06) Chinese\n",
      "\n",
      "> Satoshi\n",
      "Probability (0.06) Scottish\n",
      "Probability (0.06) Portuguese\n",
      "Probability (0.06) Vietnamese\n",
      "\n",
      "> Cui\n",
      "Probability (0.06) Portuguese\n",
      "Probability (0.06) Irish\n",
      "Probability (0.06) Scottish\n",
      "\n",
      "> Zhuang\n",
      "Probability (0.06) Chinese\n",
      "Probability (0.06) Irish\n",
      "Probability (0.06) Russian\n",
      "\n",
      "> Xue\n",
      "Probability (0.06) Scottish\n",
      "Probability (0.06) Chinese\n",
      "Probability (0.06) Irish\n",
      "\n",
      "> Wang\n",
      "Probability (0.07) Chinese\n",
      "Probability (0.06) Irish\n",
      "Probability (0.06) Russian\n"
     ]
    }
   ],
   "source": [
    "def predict(input_line, n_predictions=3):\n",
    "    print('\\n> %s' % input_line)\n",
    "    with torch.no_grad():\n",
    "        output = evaluate(lineToTensor(input_line))\n",
    "        output = torch.nn.functional.softmax(output, dim=1)\n",
    "\n",
    "        # Get top N categories\n",
    "        topv, topi = output.topk(n_predictions, 1, True)\n",
    "        predictions = []\n",
    "\n",
    "        for i in range(n_predictions):\n",
    "            value = topv[0][i].item()\n",
    "            category_index = topi[0][i].item()\n",
    "            print('Probability (%.2f) %s' % (value, all_categories[category_index]))\n",
    "            predictions.append([value, all_categories[category_index]])\n",
    "\n",
    "predict('Dovesky')\n",
    "predict('Jackson')\n",
    "predict('Satoshi')\n",
    "predict(\"Cui\")\n",
    "predict(\"Zhuang\")\n",
    "predict(\"Xue\")\n",
    "predict(\"Wang\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Exercises\n",
    "==============================================================================================================\n",
    "\n",
    "1、在第`1.4.3`节中编写代码，打印出训练过程当中，loss值的变化，（loss已经在训练过程中记录，找出并画成曲线即可，你也可以选择自己修改训练过程中的代码，自己实现重新画成更加漂亮的曲线。）   \n",
    "   \n",
    "2、在第`1.6`节，将自己的姓名的每一个字的拼音输入进去，进行预测，看模型预测是否正确（保留上面的预测结果）。如名字为`张三`，则输入`predict('Zhang')`和`predict('San')`.   \n",
    "\n",
    "3、编辑该cell，在下面一行回答问题。当前的`BaseRNN`中使用的激活函数修改成`relu`（在第1.3小节中），然后重新训练模型，看性能是否有变化。\n",
    "\n",
    "当前的激活函数是（      ），训练集准确率为：（            ）验证集的准确率为：（                 ）  \n",
    "激活函数为`Relu`时的训练准确率为：（            ）验证集的准确率为：（                 ）\n",
    "\n",
    "4、编辑该cell，在下面一行记录当前的模型的准确率，然后，回到1.3中，编辑`DeeperRNN`类，使其比`BaseRNN`类更深一层，并且重新训练这个更深的网络，看新能是否有提高（记得去掉`DeeperRNN`那一个cell中的最后两行的注释）。对于RNN的隐藏层的增加可阅读下一个cell的补充理解\n",
    "\n",
    "`BaseRNN`的训练集准确率为：（            ）验证集的准确率为：（                 ）  \n",
    "`DeeperRNN`的训练准确率为：（            ）验证集的准确率为：（                 ）    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "补充材料：    \n",
    ">RNNs are neural networks and everything works monotonically better (if done right) if you put on your deep learning hat and start stacking models up like pancakes. For instance, we can form a 2-layer recurrent network as follows:\n",
    "``` python\n",
    "y1 = rnn1.step(x)\n",
    "y = rnn2.step(y1)\n",
    "```\n",
    ">In other words we have two separate RNNs: One RNN is receiving the input vectors and the second RNN is receiving the output of the first RNN as its input. Except neither of these RNNs know or care - it’s all just vectors coming in and going out, and some gradients flowing through each module during backpropagation.\n",
    "\n",
    "原文阅读点击[这里](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generating Names with a Character-Level RNN\n",
    "******************************************************    \n",
    "We are still hand-crafting a small RNN with a few linear layers. The big\n",
    "difference is instead of predicting a category after reading in all the\n",
    "letters of a name, we input a category and output one letter at a time.\n",
    "Recurrently predicting characters to form language (this could also be\n",
    "done with words or other higher order constructs) is often referred to\n",
    "as a \"language model\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Creating the Network\n",
    "\n",
    "\n",
    "This network extends [the last tutorial's RNN](#Creating-the-Network)\n",
    "with an extra argument for the category tensor, which is concatenated\n",
    "along with the others. The category tensor is a one-hot vector just like\n",
    "the letter input.\n",
    "\n",
    "We will interpret the output as the probability of the next letter. When\n",
    "sampling, the most likely output letter is used as the next input\n",
    "letter.\n",
    "\n",
    "I added a second linear layer ``o2o`` (after combining hidden and\n",
    "output) to give it more muscle to work with. There's also a dropout\n",
    "layer, which [randomly zeros parts of its\n",
    "input](https://arxiv.org/abs/1207.0580) with a given probability\n",
    "(here 0.2) and is usually used to fuzz inputs to prevent overfitting.\n",
    "Here we're using it towards the end of the network to purposely add some\n",
    "chaos and increase sampling variety.\n",
    "\n",
    "![figure](https://karpathy.github.io/assets/rnn/charseq.jpeg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GenerateRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(GenerateRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.c2h = nn.Linear(n_categories, hidden_size)\n",
    "        self.i2h = nn.Linear(input_size,   hidden_size)\n",
    "        self.h2h = nn.Linear(hidden_size,  hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.h2o = nn.Linear(hidden_size,  output_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, category, input, hidden):\n",
    "        c2h = self.c2h(category)\n",
    "        i2h = self.i2h(input)\n",
    "        h2h = self.h2h(hidden)\n",
    "        \n",
    "        hidden = self.activation(c2h+i2h+h2h)\n",
    "        \n",
    "\n",
    "        dropout = self.dropout(self.h2o(hidden))\n",
    "        output = self.softmax(dropout)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Training\n",
    "#### 2.2.1 Preparing for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Random item from a list\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "# Get a random category and random line from that category\n",
    "def randomTrainingPair():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    return category, line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each timestep (that is, for each letter in a training word) the\n",
    "inputs of the network will be\n",
    "``(category, current letter, hidden state)`` and the outputs will be\n",
    "``(next letter, next hidden state)``. So for each training set, we'll\n",
    "need the category, a set of input letters, and a set of output/target\n",
    "letters.\n",
    "\n",
    "Since we are predicting the next letter from the current letter for each\n",
    "timestep, the letter pairs are groups of consecutive letters from the\n",
    "line - e.g. for ``\"ABCD<EOS>\"`` we would create (\"A\", \"B\"), (\"B\", \"C\"),\n",
    "(\"C\", \"D\"), (\"D\", \"EOS\").\n",
    "\n",
    "![figure](https://i.imgur.com/JH58tXY.png)\n",
    "\n",
    "The category tensor is a [one-hot\n",
    "tensor](https://en.wikipedia.org/wiki/One-hot) of size\n",
    "``<1 x n_categories>``. When training we feed it to the network at every\n",
    "timestep - this is a design choice, it could have been included as part\n",
    "of initial hidden state or some other strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot vector for category\n",
    "def categoryTensor(category):\n",
    "    li = all_categories.index(category)\n",
    "    tensor = torch.zeros(1, n_categories)\n",
    "    tensor[0][li] = 1\n",
    "    return tensor\n",
    "\n",
    "# One-hot matrix of first to last letters (not including EOS) for input\n",
    "def inputTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li in range(len(line)):\n",
    "        letter = line[li]\n",
    "        tensor[li][0][all_letters.find(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# LongTensor of second letter to end (EOS) for target\n",
    "def targetTensor(line):\n",
    "    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n",
    "    letter_indexes.append(n_letters - 1) # EOS\n",
    "    return torch.LongTensor(letter_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience during training we'll make a ``randomTrainingExample``\n",
    "function that fetches a random (category, line) pair and turns them into\n",
    "the required (category, input, target) tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make category, input, and target tensors from a random category, line pair\n",
    "def randomTrainingExample():\n",
    "    category, line = randomTrainingPair()\n",
    "    category_tensor = categoryTensor(category)\n",
    "    input_line_tensor = inputTensor(line)\n",
    "    target_line_tensor = targetTensor(line)\n",
    "    return category_tensor, input_line_tensor, target_line_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Training the Network\n",
    "\n",
    "In contrast to classification, where only the last output is used, we\n",
    "are making a prediction at every step, so we are calculating loss at\n",
    "every step.\n",
    "\n",
    "The magic of autograd allows you to simply sum these losses at each step\n",
    "and call backward at the end.\n",
    "\n",
    "\n",
    "For the loss function ``nn.NLLLoss`` is appropriate, since the last\n",
    "layer of the RNN is ``nn.LogSoftmax``.\n",
    "\n",
    "For the different between `nn.NLLLoss` and the `nn.LogSoftmax`, we could Look\n",
    "at the source code, the most import line is quote as follow:\n",
    "\n",
    "``` python\n",
    " def cross_entropy(input, target):\n",
    "    return nll_loss(log_softmax(input, 1))\n",
    "\n",
    "```\n",
    "\n",
    "In a word, Cross entropy combines `log_softmax` and `nll_loss` in a single function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "\n",
    "learning_rate = 0.0005\n",
    "\n",
    "def train(category_tensor, input_line_tensor, target_line_tensor):\n",
    "    target_line_tensor.unsqueeze_(-1)\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # Take care of the loss function, \n",
    "    # it could be visualized as the following figure \n",
    "    for i in range(input_line_tensor.size(0)):\n",
    "        output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)\n",
    "        l = criterion(output, target_line_tensor[i])\n",
    "        loss += l\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for p in rnn.parameters():\n",
    "        if hasattr(p.grad, \"data\"):\n",
    "            p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.item() / input_line_tensor.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function could be shown as the following picture, the picture is quoted from [here]      \n",
    "\n",
    "++++++++++++++++++++++++++++++++++= 画小图\n",
    "\n",
    "(http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf)\n",
    "![figure](./images/backpro.png)    \n",
    "\n",
    "\n",
    "To keep track of how long training takes I am adding a timeSince(timestamp) function which returns a human readable string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is business as usual - call train a bunch of times and wait a few minutes, printing the current time and loss every print_every examples, and keeping store of an average loss per plot_every examples in all_losses for plotting later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 10s (5000 5%) 3.1451\n",
      "0m 21s (10000 10%) 3.0309\n",
      "0m 32s (15000 15%) 2.9633\n",
      "0m 45s (20000 20%) 2.9155\n",
      "0m 57s (25000 25%) 2.8781\n",
      "1m 9s (30000 30%) 2.8478\n",
      "1m 21s (35000 35%) 2.8222\n",
      "1m 31s (40000 40%) 2.7999\n",
      "1m 42s (45000 45%) 2.7798\n",
      "1m 52s (50000 50%) 2.7630\n",
      "2m 3s (55000 55%) 2.7483\n",
      "2m 15s (60000 60%) 2.7341\n",
      "2m 26s (65000 65%) 2.7205\n",
      "2m 37s (70000 70%) 2.7071\n",
      "2m 49s (75000 75%) 2.6947\n",
      "3m 1s (80000 80%) 2.6833\n",
      "3m 13s (85000 85%) 2.6721\n",
      "3m 25s (90000 90%) 2.6622\n",
      "3m 37s (95000 95%) 2.6533\n",
      "3m 49s (100000 100%) 2.6441\n"
     ]
    }
   ],
   "source": [
    "rnn = GenerateRNN(n_letters, 128, n_letters)\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 5000\n",
    "plot_every = 500\n",
    "all_losses = []\n",
    "total_loss = 0 # Reset every plot_every iters\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    output, loss = train(*randomTrainingExample())\n",
    "    total_loss += loss\n",
    "\n",
    "    if iter % print_every == 0:\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, total_loss/iter))\n",
    "\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(total_loss / iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Plotting the Losses\n",
    "\n",
    "Plotting the historical loss from all\\_losses shows the network\n",
    "learning:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9ba80475c0>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Sampling the Network\n",
    "\n",
    "To sample we give the network a letter and ask what the next one is,\n",
    "feed that in as the next letter, and repeat until the EOS token.\n",
    "\n",
    "-  Create tensors for input category, starting letter, and empty hidden\n",
    "   state\n",
    "-  Create a string ``output_name`` with the starting letter\n",
    "-  Up to a maximum output length,\n",
    "\n",
    "   -  Feed the current letter to the network\n",
    "   -  Get the next letter from highest output, and next hidden state\n",
    "   -  If the letter is EOS, stop here\n",
    "   -  If a regular letter, add to ``output_name`` and continue\n",
    "\n",
    "-  Return the final name\n",
    "\n",
    "**Note:**\n",
    "   Rather than having to give it a starting letter, another\n",
    "   strategy would have been to include a \"start of string\" token in\n",
    "   training and have the network choose its own starting letter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chavers\n",
      "Charin\n",
      "Charink\n",
      "Carin\n",
      "Charin\n",
      "Chaver\n",
      "Charin\n",
      "Zakin\n",
      "Zinovevs\n",
      "Zarin\n",
      "Zakin\n",
      "Zakilov\n",
      "Zakin\n",
      "Zakin\n",
      "Zakin\n",
      "Zakinov\n",
      "Yovel\n",
      "Yakinov\n",
      "Yakivan\n",
      "Yarin\n",
      "Yakilovev\n",
      "Yovero\n",
      "Yakinok\n",
      "Yakin\n",
      "Yoveri\n",
      "Yovon\n",
      "\n",
      "\n",
      "Chere\n",
      "Chere\n",
      "Chere\n",
      "Chere\n",
      "Chere\n",
      "Chere\n",
      "Changer\n",
      "Zeren\n",
      "Zeuner\n",
      "Zerer\n",
      "Zerrin\n",
      "Zeure\n",
      "Zare\n",
      "Zarterra\n",
      "Zong\n",
      "Zerer\n",
      "Yereng\n",
      "Yerter\n",
      "Yangerten\n",
      "Yereng\n",
      "Yereng\n",
      "Yanger\n",
      "Yeren\n",
      "Yore\n",
      "Yeure\n",
      "Yerrer\n",
      "\n",
      "\n",
      "Canera\n",
      "Cara\n",
      "Cara\n",
      "Chara\n",
      "Chara\n",
      "Cara\n",
      "Caler\n",
      "Zaner\n",
      "Zanera\n",
      "Zane\n",
      "Zara\n",
      "Zane\n",
      "Zena\n",
      "Zanera\n",
      "Zaner\n",
      "Zane\n",
      "Yanel\n",
      "Yanga\n",
      "Yanel\n",
      "Yaner\n",
      "Yaner\n",
      "Yaner\n",
      "Yana\n",
      "Yara\n",
      "Yana\n",
      "Yaner\n",
      "\n",
      "\n",
      "Chin\n",
      "Chan\n",
      "Cha\n",
      "Can\n",
      "Chan\n",
      "Chan\n",
      "Cho\n",
      "Zhin\n",
      "Zang\n",
      "Zhi\n",
      "Zhi\n",
      "Zha\n",
      "Zhai\n",
      "Zhai\n",
      "Zha\n",
      "Zan\n",
      "Yan\n",
      "Yan\n",
      "Yin\n",
      "Yang\n",
      "Yan\n",
      "Yan\n",
      "Yan\n",
      "Yane\n",
      "You\n",
      "Yan\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_length = 20\n",
    "\n",
    "# Sample from a category and starting letter\n",
    "def sample(category, start_letter='A'):\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        category_tensor = categoryTensor(category)\n",
    "        input = inputTensor(start_letter)\n",
    "        hidden = rnn.initHidden()\n",
    "\n",
    "        output_name = start_letter\n",
    "\n",
    "        for i in range(max_length):\n",
    "            output, hidden = rnn(category_tensor, input[0], hidden)\n",
    "            topv, topi = output.topk(1)\n",
    "            topi = topi[0][0]\n",
    "            if topi == n_letters - 1:\n",
    "                break\n",
    "            else:\n",
    "                letter = all_letters[topi]\n",
    "                output_name += letter\n",
    "            input = inputTensor(letter)\n",
    "\n",
    "        return output_name\n",
    "\n",
    "# Get multiple samples from one category and multiple starting letters\n",
    "def samples(category, start_letters='ABC'):\n",
    "    for start_letter in start_letters:\n",
    "        print(sample(category, start_letter))\n",
    "    print(\"\\n\")\n",
    "\n",
    "rnn.train()\n",
    "###################################  为什么要dropout的\n",
    "###################################  根据概率值随机选。 importance sampling.\n",
    "samples('Russian', 'CCCCCCCZZZZZZZZZYYYYYYYYYY')\n",
    "\n",
    "samples('German',  'CCCCCCCZZZZZZZZZYYYYYYYYYY')\n",
    "\n",
    "samples('Spanish', 'CCCCCCCZZZZZZZZZYYYYYYYYYY')\n",
    "\n",
    "samples('Chinese', 'CCCCCCCZZZZZZZZZYYYYYYYYYY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chanevski\n",
      "Yoser\n",
      "Yovarevev\n",
      "\n",
      "\n",
      "Cherr\n",
      "Ceull\n",
      "Yeren\n",
      "\n",
      "\n",
      "Chenanez\n",
      "Chellanar\n",
      "Zona\n",
      "Zanera\n",
      "Yeuna\n",
      "Yerie\n",
      "\n",
      "\n",
      "Youn\n",
      "Yoig\n",
      "Yin\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "max_length = 20\n",
    "\n",
    "# Sample from a category and starting letter\n",
    "def sample(category, start_letter='A'):\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        category_tensor = categoryTensor(category)\n",
    "        input = inputTensor(start_letter)\n",
    "        hidden = rnn.initHidden()\n",
    "\n",
    "        output_name = start_letter\n",
    "\n",
    "        for i in range(max_length):\n",
    "            output, hidden = rnn(category_tensor, input[0], hidden)\n",
    "            #print(output.shape)\n",
    "            topv, topi = output.topk(3)\n",
    "            index = np.random.choice([0, 1, 2], 1, p=[0.6, 0.3, 0.1])\n",
    "            #print(index)\n",
    "            #print(topi.shape)\n",
    "            topi = topi[0][index]\n",
    "            if topi == n_letters - 1:\n",
    "                break\n",
    "            else:\n",
    "                letter = all_letters[topi]\n",
    "                output_name += letter\n",
    "            input = inputTensor(letter)\n",
    "\n",
    "        return output_name\n",
    "\n",
    "# Get multiple samples from one category and multiple starting letters\n",
    "def samples(category, start_letters='ABC'):\n",
    "    for start_letter in start_letters:\n",
    "        print(sample(category, start_letter))\n",
    "    print(\"\\n\")\n",
    "\n",
    "rnn.train()\n",
    "###################################  为什么要dropout的\n",
    "###################################  根据概率值随机选。 importance sampling.\n",
    "samples('Russian', 'CYY')\n",
    "\n",
    "samples('German', 'CCY')\n",
    "\n",
    "samples('Spanish', 'CCZZYY')\n",
    "\n",
    "samples('Chinese', 'YYY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Exercises\n",
    "\n",
    "-  反复输入自己名字的首字母，观察网络的生成的名字是否一样\n",
    "-  为什么这个模型这个模型训练好了之后，输入同样的参数会产生不一样的结果\n",
    "-  什么时候终止预测新的字符？\n",
    "-  实现impantance sampling的方式，使得生成的名字具有一定的差异性。\n",
    "   \n",
    "下一周：１、自己实现ｌｓｔｍ。２、翻译；４、探索各个ｇａｔｅ的作用（开关状态），预测过程中每个gate的激活状态，5 Attention。　　　\n",
    "\n",
    "这一周：改成简单的模型，按照课件的图表。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
